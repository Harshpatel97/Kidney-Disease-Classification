{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Kidney-Disease-Classification\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    data: Path\n",
    "    model: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KidneyDisease.constants import *\n",
    "from KidneyDisease.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_trainer_config(self)-> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        \n",
    "        create_directories([config.model])\n",
    "        \n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            data=config.data,\n",
    "            model=config.model\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\kidney\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch, torchvision\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from KidneyDisease.utils.helper_functions import *\n",
    "# Setting up the device agnostic code.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def trainer(self):\n",
    "        # Location of the data\n",
    "        loc = self.config.data\n",
    "        \n",
    "        # Transforming the data and turning into tensor format\n",
    "        data_transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Custom data loader for pytorch\n",
    "        data = ImageFolderCustom(targ_dir=loc,\n",
    "                                 transform=data_transform)\n",
    "        \n",
    "        ## Splitting the data into train and test data for training.\n",
    "        train_size = int(0.70 * len(data))\n",
    "        test_size = len(data) - train_size\n",
    "        \n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "        # Setup batch size and number of workers\n",
    "        BATCH_SIZE = 64\n",
    "        print(f\"Creating DataLoader's with batch size {BATCH_SIZE}\")\n",
    "\n",
    "        # Turn train and test custom Dataset's into DataLoader's\n",
    "        from torch.utils.data import DataLoader\n",
    "        train_dataloader_custom = DataLoader(dataset=train_dataset, # use custom created train Dataset\n",
    "                                            batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                            num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                            shuffle=True) # shuffle the data?\n",
    "\n",
    "        test_dataloader_custom = DataLoader(dataset=test_dataset, # use custom created test Dataset\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_workers=0,\n",
    "                                            shuffle=False) # don't usually need to shuffle testing data    \n",
    "                \n",
    "        # 1. Setup pretrained EffNetB2 weights\n",
    "        effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "        # 2. Get EffNetB2 transforms\n",
    "        effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "        # 3. Setup pretrained model\n",
    "        effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "        # 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "        for param in effnetb2.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 5. Update the classifier head\n",
    "        effnetb2.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "            nn.Linear(in_features=1408, # keep in_features same\n",
    "                    out_features=4)) # change out_features to suit our number of classes   \n",
    "                    \n",
    "        # Define loss and optimizer\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                                    lr=1e-3) \n",
    "\n",
    "        # Start the timer\n",
    "        from timeit import default_timer as timer\n",
    "        start_time = timer()\n",
    "\n",
    "        # Setup training and save the results\n",
    "        results = train(model=effnetb2.to(device),\n",
    "                            train_dataloader=train_dataloader_custom,\n",
    "                            test_dataloader=test_dataloader_custom,\n",
    "                            optimizer=optimizer,\n",
    "                            loss_fn=loss_fn,\n",
    "                            epochs=1)\n",
    "\n",
    "        # End the timer and print out how long it took\n",
    "        end_time = timer()\n",
    "        print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") \n",
    "        \n",
    "        torch.save(effnetb2, 'artifacts/models/model_1.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-13 18:47:14,761: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-10-13 18:47:14,762: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-10-13 18:47:14,762: INFO: common: created directory at: artifacts]\n",
      "[2023-10-13 18:47:14,762: INFO: common: created directory at: artifacts/models]\n",
      "Creating DataLoader's with batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:34<00:00, 574.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7435 | train_acc: 0.7394 | test_loss: 0.4740 | test_acc: 0.8647\n",
      "[INFO] Total training time: 574.761 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.trainer()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
